# -*- coding: utf-8 -*-
"""
Created on Wed Dec 13 00:10:30 2017

@author: wangyu
"""

from sklearn.datasets import load_svmlight_file
import numpy as np
import matplotlib.pyplot as plt

feature_num = 123
methods=["SGD","NAG","RMSProp","AdaDelta","Adam"]


def sigmoid(z):
    return 1/(1+np.exp(-1.0*z))


def loss_function(Weight, X,y):
    L = 0
    N = y.shape[0]
    temp=1-y*np.matmul(X,Weight)
    L = sum(np.maximum(0, temp))
    loss =0.5 * np.matmul(Weight.T, Weight)[0][0] + (L * parm.get("C"))/N
    return loss


def gradient(Weight,X,y):
    L = np.zeros((124,1))
    temp=1-y*np.matmul(X,Weight)
    temp=np.maximum(temp/np.abs(temp),0)

    y=y*temp
    L=-np.matmul(X.T,y)
    return (parm.get("C") * L) + Weight


def SGD(Weight,X,y):
    l_rate = 0.01
    Weight -= l_rate*gradient(Weight,X,y)
    return Weight


def NAG(Weight,X,y):
    global list
    global parm
    l_rate = 0.012
    #momentum=list.get('NAG')
    m = np.zeros([feature_num + 1, 1])
    gamma = 0.9
    grad=gradient(Weight-(gamma*m),X,y)
    update_ = l_rate*grad+ m*gamma
    m=update_
    Weight -= update_
    return Weight


def RMSProp(Weight,X,y):
    #G=list.get("RMSProp")
    m=np.zeros([feature_num+1,1])
    gamma = 0.8
    Epsilon = 10e-8
    l_rate = 0.015
    grad=gradient(Weight,X,y)
    G=m+(1-gamma)*grad**2
    Weight-=l_rate*grad/np.sqrt(G+Epsilon)
    m=G
    return Weight


def AdaDelta(Weight,X,y):
    m = np.zeros([feature_num+1,1])
    n = np.zeros([feature_num+1,1])
    gamma = 0.98
    Epsilon = 10e-6
    grad=gradient(Weight,X,y)
    EG=gamma*m+(1-gamma)*grad**2
    
    delta=-1*grad*np.sqrt(n+Epsilon)/np.sqrt(EG+Epsilon)
    EX=gamma*n+(1-gamma)*delta**2
    m = EG
    n = EX
    Weight += delta
    return Weight


def Adam(Weight,X,y):
    beta = 0.8
    gamma = 0.98
    Epsilon = 10e-6
    l_rate = 0.01
    _m = np.zeros([feature_num+1,1])
    _g = np.zeros([feature_num+1,1])
    _t = 0
    t=_t+1
    _t=t
    grad=gradient(Weight,X,y)
    M=beta*_m+(1-beta)*grad
    _m=M
    G=gamma*_g+(1-gamma)*grad**2
    _g=G
    M_bias=M/(1-beta**t)
    G_bias=G/(1-gamma**t)
    Weight -= l_rate*M_bias/(np.sqrt(G_bias)+Epsilon)
    return Weight


def opitimizer(W,X_train,y_train,method):
    if method=="SGD":
        return SGD(W,X_train,y_train)
    if method=="NAG":
        return NAG(W,X_train,y_train)
    if method=="RMSProp":
        return RMSProp(W,X_train,y_train)
    if method=="AdaDelta":
        return AdaDelta(W,X_train,y_train)
    if method=="Adam":
        return Adam(W,X_train,y_train)


def getdata():
    X_train, y_train = load_svmlight_file("C:/Users/wangyu/Desktop/大三/机器学习/实验/a9a.txt")
    datasize,features=X_train.shape
    X_train=np.c_[np.ones(len(X_train.toarray())), X_train.toarray()]
    for i in range(0, len(y_train)):
        if y_train[i] == -1:
            y_train[i] = 0
    X_test,y_test=load_svmlight_file("C:/Users/wangyu/Desktop/大三/机器学习/实验/a9a2.txt")
    X_test=np.c_[X_test.toarray(),np.zeros(len(X_test.toarray()))]
    X_test=np.c_[np.ones(len(X_test)),X_test]
    for i in range(0, len(y_test)):
        if y_test[i] == -1:
            y_test[i] = 0
    y_train = y_train.reshape([len(y_train), 1])
    y_test = y_test.reshape([len(y_test), 1])
    X_train,y_train=shuffle(X_train,y_train)
    X_test,y_test=shuffle(X_test,y_test)
    return X_train,y_train,X_test,y_test,datasize,features


def get_sub_batch(batch_count,X,y,data_size):
    if (1+batch_count)*batch_size<=data_size:
        return X[batch_count*batch_size:(batch_count + 1) * batch_size],y[batch_count*batch_size:(batch_count + 1) * batch_size]
    else:
        return X[batch_count*batch_size:data_size],y[batch_count*batch_size:data_size]


def shuffle(X,y):
    rng_state = np.random.get_state()
    np.random.shuffle(X)
    np.random.set_state(rng_state)
    np.random.shuffle(y)
    return X,y


def LinearClassif():
    X_train, y_train, X_test, y_test, data_size, features_num = getdata()
    plt.xlabel('iters')
    plt.ylabel('Loss')

    for method in methods:
        W = np.random.rand(features_num + 1, 1)
        iter_ = []
        error = []
        num = 0
        for j in range(2):
            for i in range(0, int(data_size / batch_size ) + 1):
                iter_.append(num)
                X,y=get_sub_batch(i,X_train,y_train,data_size)
                W=opitimizer(W,X,y,method)
                error.append(loss_function(W,X_test,y_test))
                num+=1
        plt.plot(iter_, error, label=method)
    plt.legend()
    plt.show()


LinearClassif()