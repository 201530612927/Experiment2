# -*- coding: utf-8 -*-
"""
Created on Wed Dec 13 00:10:30 2017

@author: wangyu
"""

from sklearn.datasets import load_svmlight_file
from sklearn import preprocessing

from sklearn.model_selection import train_test_split
import numpy as np
import matplotlib.pyplot as plt


#feature_num=123
batch_size=128
SGD_methods=["SGD","NAG","RMSProp","AdaDelta","Adam"]
parm={"SGD":{"learning rate":0.008},\
      "NAG":{"learning rate":0.008,"Gamma":0.92},\
      "RMSProp":{"learning rate":0.008,"Gamma":0.9,"Epsilon":10e-8},\
      "AdaDelta":{"Gamma":0.98,"Epsilon":10e-6},\
      "Adam":{"Beta":0.8,"Gamma":0.85,"learning rate":0.008,"Epsilon":10e-8}}
list={"NAG":np.zeros([feature_num + 1, 1]),\
           "RMSProp":np.zeros([feature_num+1,1]),\
           "AdaDelta":{"EG":np.zeros([feature_num+1,1]),"EX":np.zeros([feature_num+1,1])},\
           "Adam":{"M":np.zeros([feature_num+1,1]),"G":np.zeros([feature_num+1,1]),"t":0}}

#定义sigmoid函数
def sigmoid(z):
    return 1/(1+np.exp(-1.0*z))

#定义损失函数
def loss_function(Weight, X,y):
    l = np.matmul(X, Weight)
    loss = -np.mean(y * np.log(sigmoid(l)) + (1 - y) * np.log(1 - sigmoid(l)))
    return loss

#求解梯度
def gradient(Weight,X,y):
    l = np.matmul(X, Weight)
    output = sigmoid(l)
    error = output - y
    grad = np.matmul(X.transpose(), error) / y.shape[0]
    return grad

def SGD(Weight,X,y):
    W-=parm.get("SGD").get("learning rate")*gradient(Weight,X,y)
    return Weight

def NAG(Weight,X,y):
    global list
    global parm
    momentum=list.get('NAG')
    Gamma=parm.get("NAG").get("Gamma")
    grad=gradient(Weight-(Gamma*momentum),X,y)
    update_momentum = momentum * Gamma+ grad * parm.get("NAG").get("learning rate")
    list["NAG"]=update_momentum
    Weight-=update_momentum
    return Weight

def RMSProp(Weight,X,y):
    G=list.get("RMSProp")
    Gamma =parm.get("RMSProp").get("Gamma")
    Epsilon=parm.get("RMSProp").get("Epsilon")
    learning_rate=parm.get("RMSProp").get("learning rate")
    grad=gradient(W,X_train,y_train)
    G=G+(1-Gamma)*grad**2
    list["RMSProp"]=G
    Weight -= learning_rate*grad/np.sqrt(G+Epsilon)
    return Weight

def AdaDelta(Weight,X,y):
    EG=list.get("AdaDelta").get("EG")
    EX=list.get("AdaDelta").get("EX")
    Gamma=parm.get("AdaDelta").get("Gamma")
    Epsilon=parm.get("AdaDelta").get("Epsilon")
    grad=gradient(Weight,X,y)
    EG=Gamma*EG+(1-Gamma)*grad**2
    list.get("AdaDelta")["EG"]=EG
    delta=-1*grad*np.sqrt(EX+Epsilon)/np.sqrt(EG+Epsilon)
    EX=Gamma*EX+(1-Gamma)*delta**2
    list.get("AdaDelta")["EX"]=EX
    Weight+=delta
    return Weight

def Adam(Weight,X,y):
    Beta=parm.get("Adam").get("Beta")
    Gamma=parm.get("Adam").get("Gamma")
    Epsilon=parm.get("Adam").get("Epsilon")
    learning_rate=parm.get("Adam").get("learning rate")
    M=list.get("Adam").get("M")
    G=list.get("Adam").get("G")
    t=list.get("Adam").get("t")
    t=t+1
    list.get("Adam")["t"]=t
    grad=gradient(Weight,X,y)
    M=Beta*M+(1-Beta)*grad
    list.get("Adam")["M"]=M
    G=Gamma*G+(1-Gamma)*grad**2
    list.get("Adam")["G"]=G
    M_bias=M/(1-Beta**t)
    G_bias=G/(1-Gamma**t)
    Weight -= learning_rate*M_bias/(np.sqrt(G_bias)+Epsilon)
    return Weight

def opitimizer(Weight,X,y,method):
    if method=="SGD":
        return SGD(Weight,X,y)
    if method=="NAG":
        return NAG(Weight,X,y)
    if method=="RMSProp":
        return RMSProp(Weight,X,y)
    if method=="AdaDelta":
        return AdaDelta(Weight,X,y)
    if method=="Adam":
        return Adam(Weight,X,y)

def getdata():
    X_train, y_train = load_svmlight_file("C:/Users/wangyu/Desktop/大三/机器学习/实验/a9a.txt")
    datasize,features=X_train.shape
    X_train=np.c_[np.ones(len(X_train.toarray())), X_train.toarray()]
    for i in range(0, len(y_train)):
        if y_train[i] == -1:
            y_train[i] = 0
    X_test,y_test=load_svmlight_file("C:/Users/wangyu/Desktop/大三/机器学习/实验/a9a2.txt")
    X_test=np.c_[X_test.toarray(),np.zeros(len(X_test.toarray()))]
    X_test=np.c_[np.ones(len(X_test)),X_test]
    for i in range(0, len(y_test)):
        if y_test[i] == -1:
            y_test[i] = 0
    y_train = y_train.reshape([len(y_train), 1])
    y_test = y_test.reshape([len(y_test), 1])
    X_train,y_train=shuffle(X_train,y_train)
    X_test,y_test=shuffle(X_test,y_test)
    return X_train,y_train,X_test,y_test,datasize,features

def get_sub_batch(batch_count,X,y,data_size):
    if (1+batch_count)*batch_size<=data_size:
        return X[batch_count*batch_size:(batch_count + 1) * batch_size],y[batch_count*batch_size:(batch_count + 1) * batch_size]
    else:
        return X[batch_count*batch_size:data_size],y[batch_count*batch_size:data_size]

def shuffle(X,y):
    rng_state = np.random.get_state()
    np.random.shuffle(X)
    np.random.set_state(rng_state)
    np.random.shuffle(y)
    return X,y

def LogicReg():
    X_train, y_train, X_test, y_test, data_size, features_num = getdata()
    plt.xlabel('iters')
    plt.ylabel('Loss')

    for method in SGD_methods:
        W = np.random.rand(features_num + 1, 1)
        iter_ = []
        error = []
        num = 0
        for j in range(2):
            for i in range(0, int(data_size / batch_size ) + 1):
                iter_.append(num)
                X,y=get_sub_batch(i,X_train,y_train,data_size)
                W=opitimizer(W,X,y,method)
                error.append(loss_function(W,X_test,y_test))
                num+=1
        plt.plot(iter_, error, label=method)
    plt.legend()
    plt.show()

LogicReg()
