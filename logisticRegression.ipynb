# -*- coding: utf-8 -*-
"""
Created on Wed Dec 13 00:10:30 2017

@author: wangyu
"""

from sklearn.datasets import load_svmlight_file
import numpy as np
import matplotlib.pyplot as plt


methods=["SGD","NAG","RMSProp","AdaDelta","Adam"]
parm={"C":0.6,\
      "SGD":{"learning rate":0.01},\
      "NAG":{"learning rate":0.008,"Gamma":0.8},\
      "RMSProp":{"learning rate":0.015,"Gamma":0.8,"Epsilon":10e-8},\
      "AdaDelta":{"Gamma":0.98,"Epsilon":10e-6},\
      "Adam":{"Beta":0.8,"Gamma":0.98,"learning rate":0.01,"Epsilon":10e-6}}
list = {"NAG":np.zeros([feature_num + 1, 1]),\
           "RMSProp":np.zeros([feature_num+1,1]),\
           "AdaDelta":{"EG":np.zeros([feature_num+1,1]),"EX":np.zeros([feature_num+1,1])},\
           "Adam":{"M":np.zeros([feature_num+1,1]),"G":np.zeros([feature_num+1,1]),"t":0}}

def sigmoid(z):
    return 1/(1+np.exp(-1.0*z))


def loss_function(Weight, X,y):
    L = 0
    N = y.shape[0]
    temp=1-y*np.matmul(X,Weight)
    L = sum(np.maximum(0, temp))
    loss =0.5 * np.matmul(Weight.T, Weight)[0][0] + (L * parm.get("C"))/N
    return loss


def gradient(W,X_train,y_train):
    L_dW = np.zeros((124,1))
    temp=1-y_train*np.matmul(X_train,W)
    temp=np.maximum(temp/np.abs(temp),0)

    y=y_train*temp
    L_dW=-np.matmul(X_train.T,y)
    return (parm.get("C") * L_dW) + W


def SGD(W,X_train,y_train):
    W-=parm.get("SGD").get("learning rate")*gradient(W,X_train,y_train)
    return W


def NAG(W,X_train,y_train):
    global list
    global parm
    momentum=list.get('NAG')
    #learning_rate=parm.get("NAG").get("learning_rate")
    Gamma=parm.get("NAG").get("Gamma")
    grad=gradient(W-(Gamma*momentum),X_train,y_train)
    update_momentum = momentum * Gamma+ grad * parm.get("NAG").get("learning rate")
    list["NAG"]=update_momentum
    W-=update_momentum
    return W


def RMSProp(W,X_train,y_train):
    G=list.get("RMSProp")
    Gamma =parm.get("RMSProp").get("Gamma")
    Epsilon=parm.get("RMSProp").get("Epsilon")
    learning_rate=parm.get("RMSProp").get("learning rate")
    grad=gradient(W,X_train,y_train)
    G=G+(1-Gamma)*grad**2
    list["RMSProp"]=G
    W-=learning_rate*grad/np.sqrt(G+Epsilon)
    return W


def AdaDelta(W,X_train,y_train):
    EG=list.get("AdaDelta").get("EG")
    EX=list.get("AdaDelta").get("EX")
    Gamma=parm.get("AdaDelta").get("Gamma")
    Epsilon=parm.get("AdaDelta").get("Epsilon")
    grad=gradient(W,X_train,y_train)
    EG=Gamma*EG+(1-Gamma)*grad**2
    list.get("AdaDelta")["EG"]=EG
    delta=-1*grad*np.sqrt(EX+Epsilon)/np.sqrt(EG+Epsilon)
    EX=Gamma*EX+(1-Gamma)*delta**2
    list.get("AdaDelta")["EX"]=EX
    W+=delta
    return W


def Adam(W,X_train,y_train):
    Beta=parm.get("Adam").get("Beta")
    Gamma=parm.get("Adam").get("Gamma")
    Epsilon=parm.get("Adam").get("Epsilon")
    learning_rate=parm.get("Adam").get("learning rate")
    M=list.get("Adam").get("M")
    G=list.get("Adam").get("G")
    t=list.get("Adam").get("t")
    t=t+1
    list.get("Adam")["t"]=t
    grad=gradient(W,X_train,y_train)
    M=Beta*M+(1-Beta)*grad
    list.get("Adam")["M"]=M
    G=Gamma*G+(1-Gamma)*grad**2
    list.get("Adam")["G"]=G
    M_bias=M/(1-Beta**t)
    G_bias=G/(1-Gamma**t)
    W-=learning_rate*M_bias/(np.sqrt(G_bias)+Epsilon)
    return W


def opitimizer(W,X_train,y_train,method):
    if method=="SGD":
        return SGD(W,X_train,y_train)
    if method=="NAG":
        return NAG(W,X_train,y_train)
    if method=="RMSProp":
        return RMSProp(W,X_train,y_train)
    if method=="AdaDelta":
        return AdaDelta(W,X_train,y_train)
    if method=="Adam":
        return Adam(W,X_train,y_train)


def getdata():
    X_train, y_train = load_svmlight_file("C:/Users/wangyu/Desktop/大三/机器学习/实验/a9a.txt")
    datasize,features=X_train.shape
    X_train=np.c_[np.ones(len(X_train.toarray())), X_train.toarray()]
    for i in range(0, len(y_train)):
        if y_train[i] == -1:
            y_train[i] = 0
    X_test,y_test=load_svmlight_file("C:/Users/wangyu/Desktop/大三/机器学习/实验/a9a2.txt")
    X_test=np.c_[X_test.toarray(),np.zeros(len(X_test.toarray()))]
    X_test=np.c_[np.ones(len(X_test)),X_test]
    for i in range(0, len(y_test)):
        if y_test[i] == -1:
            y_test[i] = 0
    y_train = y_train.reshape([len(y_train), 1])
    y_test = y_test.reshape([len(y_test), 1])
    X_train,y_train=shuffle(X_train,y_train)
    X_test,y_test=shuffle(X_test,y_test)
    return X_train,y_train,X_test,y_test,datasize,features


def get_sub_batch(batch_count,X,y,data_size):
    if (1+batch_count)*batch_size<=data_size:
        return X[batch_count*batch_size:(batch_count + 1) * batch_size],y[batch_count*batch_size:(batch_count + 1) * batch_size]
    else:
        return X[batch_count*batch_size:data_size],y[batch_count*batch_size:data_size]


def shuffle(X,y):
    rng_state = np.random.get_state()
    np.random.shuffle(X)
    np.random.set_state(rng_state)
    np.random.shuffle(y)
    return X,y


def LinearClassif():
    X_train, y_train, X_test, y_test, data_size, features_num = getdata()
    plt.xlabel('iters')
    plt.ylabel('Loss')

    for method in methods:
        W = np.random.rand(features_num + 1, 1)
        iter_ = []
        error = []
        num = 0
        for j in range(2):
            for i in range(0, int(data_size / batch_size ) + 1):
                iter_.append(num)
                X,y=get_sub_batch(i,X_train,y_train,data_size)
                W=opitimizer(W,X,y,method)
                error.append(loss_function(W,X_test,y_test))
                num+=1
        plt.plot(iter_, error, label=method)
    plt.legend()
    plt.show()


LinearClassif()