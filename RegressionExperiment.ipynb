# -*- coding: utf-8 -*-
"""
Created on Wed Dec 13 00:10:30 2017

@author: wangyu
"""

from sklearn.datasets import load_svmlight_file
from sklearn import preprocessing

from sklearn.model_selection import train_test_split
import numpy as np
import matplotlib.pyplot as plt


feature_num=123
batch_size=128
SGD_methods=["SGD","NAG","RMSProp","AdaDelta","Adam"]

#定义sigmoid函数
def sigmoid(z):
    return 1/(1+np.exp(-1.0*z))

#定义损失函数
def loss_function(Weight, X,y):
    l = np.matmul(X, Weight)
    loss = -np.mean(y * np.log(sigmoid(l)) + (1 - y) * np.log(1 - sigmoid(l)))
    return loss

#求解梯度
def gradient(Weight,X,y):
    l = np.matmul(X, Weight)
    out = sigmoid(l)
    error = out - y
    grad = np.matmul(X.transpose(), error) / y.shape[0]
    return grad

def SGD(Weight,X,y):
    l_rate = 0.008
    Weight-=l_rate*gradient(Weight,X,y)
    return Weight

def NAG(Weight,X,y):
    l_rate = 0.012
    m = np.zeros([feature_num + 1, 1])
    gamma = 0.92
    grad=gradient(Weight-(gamma*m),X,y)
    update_ = l_rate*grad+ m*gamma
    m=update_
    Weight -= update_
    return Weight

def RMSProp(Weight,X,y):
    m=np.zeros([feature_num+1,1])
    gamma = 0.8
    Epsilon = 10e-8
    l_rate = 0.015
    grad=gradient(Weight,X,y)
    G=m+(1-gamma)*grad**2
    Weight-=l_rate*grad/np.sqrt(G+Epsilon)
    m=G
    return Weight

def AdaDelta(Weight,X,y):
    m = np.zeros([feature_num+1,1])
    n = np.zeros([feature_num+1,1])
    gamma = 0.98
    Epsilon = 10e-6
    grad=gradient(Weight,X,y)
    EG=gamma*m+(1-gamma)*grad**2
    
    delta=-1*grad*np.sqrt(n+Epsilon)/np.sqrt(EG+Epsilon)
    EX=gamma*n+(1-gamma)*delta**2
    m = EG
    n = EX
    Weight += delta
    return Weight

def Adam(Weight,X,y):
    beta = 0.8
    gamma = 0.98
    Epsilon = 10e-6
    l_rate = 0.01
    _m = np.zeros([feature_num+1,1])
    _g = np.zeros([feature_num+1,1])
    _t = 0
    t=_t+1
    _t=t
    grad=gradient(Weight,X,y)
    M=beta*_m+(1-beta)*grad
    _m=M
    G=gamma*_g+(1-gamma)*grad**2
    _g=G
    M_bias=M/(1-beta**t)
    G_bias=G/(1-gamma**t)
    Weight -= l_rate*M_bias/(np.sqrt(G_bias)+Epsilon)
    return Weight


def opitimizer(Weight,X,y,method):
    if method=="SGD":
        return SGD(Weight,X,y)
    if method=="NAG":
        return NAG(Weight,X,y)
    if method=="RMSProp":
        return RMSProp(Weight,X,y)
    if method=="AdaDelta":
        return AdaDelta(Weight,X,y)
    if method=="Adam":
        return Adam(Weight,X,y)

def getdata():
    X_train, y_train = load_svmlight_file("C:/Users/wangyu/Desktop/大三/机器学习/实验/a9a.txt")
    datasize,features=X_train.shape
    X_train=np.c_[np.ones(len(X_train.toarray())), X_train.toarray()]
    for i in range(0, len(y_train)):
        if y_train[i] == -1:
            y_train[i] = 0
    X_test,y_test=load_svmlight_file("C:/Users/wangyu/Desktop/大三/机器学习/实验/a9a2.txt")
    X_test=np.c_[X_test.toarray(),np.zeros(len(X_test.toarray()))]
    X_test=np.c_[np.ones(len(X_test)),X_test]
    for i in range(0, len(y_test)):
        if y_test[i] == -1:
            y_test[i] = 0
    y_train = y_train.reshape([len(y_train), 1])
    y_test = y_test.reshape([len(y_test), 1])
    X_train,y_train=shuffle(X_train,y_train)
    X_test,y_test=shuffle(X_test,y_test)
    return X_train,y_train,X_test,y_test,datasize,features

def get_sub_batch(batch_count,X,y,data_size):
    if (1+batch_count)*batch_size<=data_size:
        return X[batch_count*batch_size:(batch_count + 1) * batch_size],y[batch_count*batch_size:(batch_count + 1) * batch_size]
    else:
        return X[batch_count*batch_size:data_size],y[batch_count*batch_size:data_size]

def shuffle(X,y):
    rng_state = np.random.get_state()
    np.random.shuffle(X)
    np.random.set_state(rng_state)
    np.random.shuffle(y)
    return X,y

def LogicReg():
    X_train, y_train, X_test, y_test, data_size, features_num = getdata()
    plt.xlabel('iters')
    plt.ylabel('Loss')

    for method in SGD_methods:
        W = np.random.rand(features_num + 1, 1)
        iter_ = []
        error = []
        num = 0
        for j in range(2):
            for i in range(0, int(data_size / batch_size ) + 1):
                iter_.append(num)
                X,y=get_sub_batch(i,X_train,y_train,data_size)
                W=opitimizer(W,X,y,method)
                error.append(loss_function(W,X_test,y_test))
                num+=1
        plt.plot(iter_, error, label=method)
    plt.legend()
    plt.show()

LogicReg()
